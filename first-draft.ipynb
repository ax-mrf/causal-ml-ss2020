{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stacking for the estimation of Treatment Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate data\n",
    "\n",
    "generate data according to the same process as Nie X. and Wager S. (2018) 'Quasi-Oracle Estimation of Heterogeneous Treatment Effects'. a library is provided by `causalml` from uber (https://github.com/uber/causalml). \n",
    "\n",
    "the goal in simulating data : to provide different examples of data generating processes, in order to conclude upon the effectiveness of stacking for treatment effects in each situation. \n",
    "\n",
    "In an experimental setup, or a situation in which we have treated and untreated data, it is necessary to estimate the underlying distribution of the 'nuisance variables', the propensity score (the liklihood, given an observation's characteristics, to be treated), and the underlying treatment effect. As such, we simulate different X, propensity, and treatment functions. \n",
    "\n",
    "`causalml` provides an implementation of each data generating function as seen in Nie & Wager, accessible through five possible modes passed to synthetic_data() :          \n",
    "\n",
    "    `       1 for difficult nuisance components and an easy treatment effect.\n",
    "            2 for a randomized trial.\n",
    "            3 for an easy propensity and a difficult baseline.\n",
    "            4 for unrelated treatment and control groups.\n",
    "            5 for a hidden confounder biasing treatment.\n",
    "`\n",
    "\n",
    "returning a dataset inclduing   : \n",
    "            - y ((n,)-array): outcome variable.\n",
    "            - X ((n,p)-ndarray): independent variables.\n",
    "            - w ((n,)-array): treatment flag with value 0 or 1.\n",
    "            - tau ((n,)-array): individual treatment effect.\n",
    "            - b ((n,)-array): expected outcome.\n",
    "            - e ((n,)-array): propensity of receiving treatment.\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feed in different models\n",
    "\n",
    "We continue in the methodology of Nie and Wager, (QUOTE), by creating R learners using various methods to estimate the underlying functions for the X variables. \n",
    "(for the moment we will always use ElasticNetPropensityModel to estimate propensity scores.)\n",
    "\n",
    "How many models can we feed in? \n",
    "\n",
    "models_dict = {'XGB': BaseRRegressor(learner=XGBRegressor()),\n",
    "               'LinearRegression':BaseRRegressor(learner=LinearRegression()),\n",
    "               'DecisionTree':BaseRRegressor(learner=DecisionTreeRegressor())}\n",
    "               \n",
    "               ##CausalTreeRegressor?\n",
    "               ##KNeighborsRegressor\n",
    "               ##SVR ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name and learner!!!!\n",
      "learner_xgb\n",
      "name and learner!!!!\n",
      "learner_lr\n",
      "name and learner!!!!\n",
      "learner_dtr\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "estimators = {'learner_xgb': BaseRRegressor(learner=XGBRegressor()),\n",
    "              'learner_lr': BaseRRegressor(learner=LinearRegression()),\n",
    "              'learner_dtr': BaseRRegressor(learner=DecisionTreeRegressor()),\n",
    "              'learner_ctr': BaseRRegressor(learner=CausalTreeRegressor()),\n",
    "              'learner_knr': BaseRRegressor(learner=KNeighborsRegressor()),\n",
    "              'learner_svr': BaseRRegressor(learner=SVR())}\n",
    "              '''\n",
    "\n",
    "from causalml.dataset import *\n",
    "from causalml.inference.meta import BaseRRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "## start a model -- feeding in XGB => to fit the nuisance model. feed in elasticpropensitymodel\n",
    "learner_xgb = BaseRRegressor(learner=XGBRegressor())\n",
    "learner_lr = BaseRRegressor(learner=LinearRegression())\n",
    "learner_dtr = BaseRRegressor(learner=DecisionTreeRegressor())\n",
    "\n",
    "###would be cool to find some other working learners, and to start messing with the params of each!!!!\n",
    "#learner_knr = BaseRRegressor(learner=KNeighborsRegressor())\n",
    "#learner_svr = BaseRRegressor(learner=SVR())\n",
    "#learner_ctr = BaseRRegressor(learner=CausalTreeRegressor())\n",
    "\n",
    "learner_xgb = BaseRRegressor(learner=XGBRegressor())\n",
    "learner_lr = BaseRRegressor(learner=LinearRegression())\n",
    "learner_dtr = BaseRRegressor(learner=DecisionTreeRegressor())\n",
    "\n",
    "estimators = {'learner_xgb': BaseRRegressor(learner=XGBRegressor()),\n",
    "              'learner_lr': BaseRRegressor(learner=LinearRegression()),\n",
    "              'learner_dtr': BaseRRegressor(learner=DecisionTreeRegressor())}\n",
    "\n",
    "predictions = get_synthetic_preds(simulate_nuisance_and_easy_treatment,\n",
    "                                               n=50000,\n",
    "                                               estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## validating accuracy : \n",
    "\"\"\"Generate a summary for predictions on synthetic data for train and holdout using specified function\n",
    "\n",
    "    Args:\n",
    "        synthetic_data_func (function): synthetic data generation function\n",
    "        n (int, optional): number of samples per simulation\n",
    "        valid_size(float,optional): validation/hold out data size\n",
    "        k (int, optional): number of simulations\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        (tuple): summary evaluation metrics of predictions for train and validation:\n",
    "\n",
    "          - summary_train (pandas.DataFrame): training data evaluation summary\n",
    "          - summary_train (pandas.DataFrame): validation data evaluation summary\n",
    "    \"\"\"\n",
    "    \n",
    "    ###YOU CAN PROBABLY FUCK WITH THE SOURCE CODE OF THIS TO MAKE A DIFFERENT VALIDATION SUMMARY, NO PROB. \n",
    "\n",
    "train_summary, validation_summary = get_synthetic_summary_holdout(simulate_nuisance_and_easy_treatment,\n",
    "                                                                  n=10000,\n",
    "                                                                  valid_size=0.2,\n",
    "                                                                  k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}