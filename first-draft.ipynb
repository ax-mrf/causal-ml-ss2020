{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stacking for the estimation of Treatment Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate data\n",
    "generate data according to the same process as Nie X. and Wager S. (2018) 'Quasi-Oracle Estimation of Heterogeneous Treatment Effects'. a library is provided by `causalml` from uber (https://github.com/uber/causalml). \n",
    "\n",
    "the goal in simulating data : to provide different examples of data generating processes, in order to conclude upon the effectiveness of stacking for treatment effects in each situation. \n",
    "\n",
    "In an experimental setup, or a situation in which we have treated and untreated data, it is necessary to estimate the underlying distribution of the 'nuisance variables', the propensity score (the likelihood, given an observation's characteristics, to be treated), and the underlying treatment effect. As such, we simulate different X, propensity, and treatment functions.\n",
    "\n",
    "`causalml` provides an implementation of each data generating function as seen in Nie & Wager, accessible through five possible modes passed to synthetic_data() :          \n",
    "\n",
    "    `       1 for difficult nuisance components and an easy treatment effect.\n",
    "            2 for a randomized trial.\n",
    "            3 for an easy propensity and a difficult baseline.\n",
    "            4 for unrelated treatment and control groups.\n",
    "            5 for a hidden confounder biasing treatment.\n",
    "\n",
    "!!!! talk here about how its useful to use the data generators from causal ml bc :\n",
    " - this allows us to generate as much data as we need in order to cross validate. we essentially always have a holdout set read to test.\n",
    " - we also need true treatment effects in order to set up a stacking pipeline, there is otherwise no way to estimate a stacking model without a true $\\tau(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 5 and input n_features is 6 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-bf75bf68fb85>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;31m#preds_rnd = reg.predict(X_pos) / reg.predict(X_neg)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m \u001B[0mthing\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_pos\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m \u001B[0;31m###reg.predict(X_pos)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/treatment_effects/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    781\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m         \u001B[0;31m# Check data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 783\u001B[0;31m         \u001B[0mX\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_X_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    784\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    785\u001B[0m         \u001B[0;31m# Assign chunk of trees to jobs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/treatment_effects/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36m_validate_X_predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    419\u001B[0m         \u001B[0mcheck_is_fitted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    420\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 421\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mestimators_\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_X_predict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_input\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    422\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    423\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.virtualenvs/treatment_effects/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001B[0m in \u001B[0;36m_validate_X_predict\u001B[0;34m(self, X, check_input)\u001B[0m\n\u001B[1;32m    394\u001B[0m         \u001B[0mn_features\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_features_\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mn_features\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 396\u001B[0;31m             raise ValueError(\"Number of features of the model must \"\n\u001B[0m\u001B[1;32m    397\u001B[0m                              \u001B[0;34m\"match the input. Model n_features is %s and \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m                              \u001B[0;34m\"input n_features is %s \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Number of features of the model must match the input. Model n_features is 5 and input n_features is 6 "
     ]
    }
   ],
   "source": [
    "### YOU COULD DO THE DATA GENERATION THING HERE WITH CAUSALML, AND THEN CREATE A PIPELINE ENTIRELY WITH SKLEARN.\n",
    "from causalml.dataset import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "### THESE NEED A RANDOM SEED IF YOU WANT TO USE THEM ON THE R LEARNER AND COMPARE.\n",
    "### nb: b is the expected outcome, (i guess without treatment? is like a counterfactual??? unsure.)\n",
    "\n",
    "\n",
    "y_a, X_a, w_a, tau_a, b_a, e_a = simulate_nuisance_and_easy_treatment()\n",
    "#y_b, X_b, w_b, tau_b, b_b, e_b = simulate_randomized_trial()\n",
    "#y_c, X_c, w_c, tau_c, b_c, e_c = simulate_easy_propensity_difficult_baseline()\n",
    "#y_e, X_e, w_e, tau_e, b_e, e_e = simulate_hidden_confounder()\n",
    "\n",
    "### the simple learners with just sklearn and some ensemble methods.\n",
    "### from this blog post : https://florianwilhelm.info/2017/04/causal_inference_propensity_score/\n",
    "\n",
    "const = np.ones(1000) ## default for causalml synthetic data gen funcs.\n",
    "df = pd.DataFrame(data=X_a)\n",
    "df['assignment'] = w_a\n",
    "df['outcome'] = y_a\n",
    "df.head()\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "X = df.iloc[:, 0:5]\n",
    "y = df['outcome']\n",
    "\n",
    "reg.fit(X, y)\n",
    "\n",
    "X_neg = pd.DataFrame(data=X)\n",
    "X_neg['assignment'] = 0\n",
    "\n",
    "X_pos = pd.DataFrame(data=X)\n",
    "X_pos['assignment'] = 1\n",
    "\n",
    "#preds_rnd = reg.predict(X_pos) / reg.predict(X_neg)\n",
    "\n",
    "thing = reg.predict(X_pos)\n",
    "###reg.predict(X_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Learner selection -\n",
    "\n",
    "## Grimmer et al, the 'T learner' and OLS.\n",
    "Nie and Wager estimate treatement effects using (among other heterogenous treatment effect estimators) the 'T-Learner', we take this learner to be the same as that which is used in Grimmer (2017)\n",
    "`causalml library also provides a T-learner as used in Grimmer (2017), where the treatment effect is estimated by simple comparison of the treated against the untreated.\n",
    "\n",
    "The T Learner fits the functions $ \\mu^*_w(x) = E(Y\\mid X = x, W = w)$ separately for the treatment and the control groups (for $w \\in{0, 1}$), and then esimates $\\tau(x) = \\mu_1(x) - \\mu_0(x)$\n",
    "The T learner (`BaseTRegressor()`) as implemented by `causalml` can be fed a machine learning model to estimate and predict treatment effects.\n",
    "it will therefore be useful to have a stacking pipeline which includes several T-Learners, over different datasets in order to examine how well a Stacked T learner does under different data generating functions.\n",
    "\n",
    "we can use this to stack estimates exactly as in Grimmer, and compare them to the R learners/DML learnings (also stacked) as in Nie, Wager\n",
    "this would answer the questions:\n",
    " - how does stacking **without** double machine learning compare to the R learner on its own? Is there an advantage to using (the best..) R learner over simple stacking? and what kind of libraries are already available for such a pipeline?\n",
    " - how much can stacking help if we do not (can not) estimate propensity scores or distinct nuisance funcitions in the dataset?\n",
    " - how does stacking an R learner compare to (the best) single DML model?\n",
    "\n",
    "the T learner does not explicitly account for propensity scores, or for the 'nuisance funciton' separate from the treatment effect.\n",
    "\n",
    "the R learner, on the other hand:\n",
    " - R leaner, DML, etc etc.\n",
    " - The proposal from Nie and Wager for using some 'out of the box models and stacking the treatment effect estimations\n",
    "\n",
    "The R learner can also be fed different models, and the results similarly stacked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feed in different models\n",
    "\n",
    "We continue in the methodology of Nie and Wager, (QUOTE), by creating R learners using various methods to estimate the underlying functions for the X variables. \n",
    "(for the moment we will always use ElasticNetPropensityModel to estimate propensity scores.)\n",
    "\n",
    "How many models can we feed in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta import BaseRRegressor, BaseTRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## can start with estimates from the T learner\n",
    "\n",
    "learner_t_xgb = BaseTRegressor(learner=XGBRegressor())\n",
    "learner_t_mlpr = BaseTRegressor(learner=MLPRegressor())\n",
    "learner_t_lr = BaseTRegressor(learner=LinearRegression())\n",
    "\n",
    "estimators = {'learner_t_xgb' : BaseTRegressor(learner=XGBRegressor()),\n",
    "              'learner_t_mlpr': BaseTRegressor(learner=MLPRegressor()),\n",
    "              'learner_t_lr' : BaseTRegressor(learner=LinearRegression())}\n",
    "\n",
    "predicitons_t_models = get_synthetic_preds(simulate_nuisance_and_easy_treatment,\n",
    "                                               n=50000,\n",
    "                                               estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "estimators = {'learner_xgb': BaseRRegressor(learner=XGBRegressor()),\n",
    "              'learner_lr': BaseRRegressor(learner=LinearRegression()),\n",
    "              'learner_dtr': BaseRRegressor(learner=DecisionTreeRegressor()),\n",
    "              'learner_ctr': BaseRRegressor(learner=CausalTreeRegressor()),\n",
    "              'learner_knr': BaseRRegressor(learner=KNeighborsRegressor()),\n",
    "              'learner_svr': BaseRRegressor(learner=SVR())}\n",
    "              '''\n",
    "###would be cool to find some other working learners, and to start messing with the params of each!!!!\n",
    "#learner_knr = BaseRRegressor(learner=KNeighborsRegressor())\n",
    "#learner_svr = BaseRRegressor(learner=SVR())\n",
    "#learner_ctr = BaseRRegressor(learner=CausalTreeRegressor())\n",
    "#'learner_sgd': BaseRRegressor(learner=SGDRegressor())\n",
    "#learner_nnr = BaseRRegressor(learner=MLPRegressor()) ##Multi-layer Perceptron regressor\n",
    "\n",
    "estimators = {'learner_xgb': BaseRRegressor(learner=XGBRegressor()),\n",
    "              'learner_lr': BaseRRegressor(learner=LinearRegression()),\n",
    "              'learner_dtr': BaseRRegressor(learner=DecisionTreeRegressor())}\n",
    "\n",
    "predictions = get_synthetic_preds(simulate_nuisance_and_easy_treatment,\n",
    "                                               n=50000,\n",
    "                                               estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### THE STADCKING PART (AGAIN.)\n",
    "\n",
    "y_stacking = predictions['generated_data']['tau']\n",
    "pred_copy = copy.deepcopy(predictions)\n",
    "pred_copy.pop('Actuals')\n",
    "pred_copy.pop('generated_data')\n",
    "\n",
    "### iterate over this better so you can add mroe models.\n",
    "x_stacking = np.vstack((predictions['learner_xgb'], predictions['learner_lr'], predictions['learner_dtr'])).T\n",
    "\n",
    "### this is using ridgeCV for the method of ols.\n",
    "### grimmer require that the coefficients of the final stacking regression sum to 1.\n",
    "### HOW CAN WE DO THIS IN SKLEARN WHILE STILL USING CAUSALML?\n",
    "\n",
    "model = sm.OLS(y_stacking, x_stacking)\n",
    "model2 = model.fit_regularized(alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False)\n",
    "print(model2.params)\n",
    "print(sum(model2.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "this is actually a sad shortcoming of the `causalml` library, that it is not possible to feed models directly into sklearn.ensemble models.\n",
    "normally an ensemble class from sklear woudl take a dict (i think..) of models and a dataset, do the entire pipeline, and then output the model.\n",
    "\n",
    "if we would like to imitate grimmer we would also need a solution to the problem that we currently dont have a way to contstrain the ensemble coefficients such that they must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_A = get_synthetic_preds(simulate_nuisance_and_easy_treatment, n=50000)\n",
    "predictions_B = get_synthetic_preds(simulate_randomized_trial, n=50000)\n",
    "predictions_C = get_synthetic_preds(simulate_easy_propensity_difficult_baseline, n=50000)\n",
    "predictions_E = get_synthetic_preds(simulate_hidden_confounder, n=50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}