{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "from opossum import UserInterface\n",
    "import matplotlib.pyplot as plt\n",
    "import causalml\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.meta import BaseRRegressor\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create datasets in an array?\n",
    "## for each thing in the data set, take each array and split into 1/3. two new arrays : Test and StackingTrain. \n",
    "# dataSetArray...\n",
    "N = 30\n",
    "k = 3\n",
    "seed= 5\n",
    "\n",
    "u = UserInterface(N, k, seed=seed, categorical_covariates = None)\n",
    "\n",
    "X=[]\n",
    "assignment=[]\n",
    "y=[]\n",
    "treatment=[]\n",
    "propensityScores=[]\n",
    "\n",
    "def splitArrays(l):\n",
    "    return np.array_split(l, 3)\n",
    "\n",
    "def addDatasets(y_gen, X_gen, assignment_gen, treatment_gen):\n",
    "    X.append(splitArrays(X_gen))\n",
    "    y.append(splitArrays(y_gen))\n",
    "    assignment.append(splitArrays(assignment_gen))\n",
    "    treatment.append(splitArrays(treatment_gen))\n",
    "    \n",
    "    \n",
    "####IMPORTANT: after generating the data, we wish to split the dataset into three parts:\n",
    "## one for training the R learner => 0\n",
    "## one for fitting the OLS stacking model => 1\n",
    "## and one for testing. => 2\n",
    "\n",
    "### because of the nature of the opossum data set output, we will use numpy arrays\n",
    "### after generating a dataset, we add the dataset to an array of datasets. each entry in this (super)array of datasets\n",
    "### contains three sub arrays. one train, one stacking, one test array. \n",
    "\n",
    "## so accessing the training dataset of the second data generating funciton is then: X[1][0]\n",
    "## accessing the treatment vector (true treatment effect) of the first data set, testing data: treatment[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## setupA => difficult nuisance com-ponents and an easy treatment effect function\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = False, \n",
    "                     assignment_prob = 'low', \n",
    "                     constant_pos = False, \n",
    "                     constant_neg = False,\n",
    "                     heterogeneous_pos = True, \n",
    "                     heterogeneous_neg = False, \n",
    "                     no_treatment = False, \n",
    "                     discrete_heterogeneous = False,\n",
    "                     treatment_option_weights = None, \n",
    "                     intensity = 10)\n",
    "\n",
    "y_A, X_A, assignment_A, treatment_A = u.output_data(binary=False, \n",
    "                                               x_y_relation = 'nonlinear_interaction')\n",
    "\n",
    "addDatasets(y_A, X_A, assignment_A, treatment_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################################\n",
    "## setupB => randomized trial\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = True, \n",
    "                     assignment_prob = 0.5, \n",
    "                     treatment_option_weights = [0.0, 0.0, 0.4, 0.6, 0.0, 0.0],\n",
    "                     intensity = 5)\n",
    "\n",
    "y_B, X_B, assignment_B, treatment_B = u.output_data(binary=False, x_y_relation = 'linear_simple')\n",
    "\n",
    "addDatasets(y_B, X_B, assignment_B, treatment_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## setupC => easy propensity score and difficult baseline\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = False, \n",
    "                     assignment_prob = 'low', \n",
    "                     constant_pos = True, \n",
    "                     constant_neg = False,\n",
    "                     heterogeneous_pos = False, \n",
    "                     heterogeneous_neg = False, \n",
    "                     no_treatment = False, \n",
    "                     discrete_heterogeneous = False,\n",
    "                     treatment_option_weights = None, \n",
    "                     intensity = 10)\n",
    "\n",
    "y_C, X_C, assignment_C, treatment_C = u.output_data(binary=False, \n",
    "                                               x_y_relation = 'nonlinear_interaction')\n",
    "\n",
    "addDatasets(y_C, X_C, assignment_C, treatment_C)\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "## setupD => unrelated treatment and control arms???\n",
    "##############################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get propensity scores using CausalML package. (scores for each dataset and save them in an array)\n",
    "propensityScores = []\n",
    "\n",
    "for x in range(3):\n",
    "    pm = ElasticNetPropensityModel(n_fold=5, random_state=42)\n",
    "    estimatedpropensityscores = pm.fit_predict(X[x], assignment[x])\n",
    "    propensityScores.append(estimatedpropensityscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## what are we estimating here.. for one dataset.\n",
    "##############################################################################################\n",
    "\n",
    "# R Learner with propensity score input\n",
    "# Calling the Base Learner class and feeding in XGB\n",
    "learner_r = BaseRRegressor(learner=XGBRegressor())\n",
    "ate_r_XGBRegressor = learner_r.estimate_ate(X=X[i], treatment = assignment[i], p=propensityScores[i], y=y[i])\n",
    "print('Using the BaseRRegressor class and using XGB:')\n",
    "print(ate_r_XGBRegressor)\n",
    "\n",
    "# Calling the Base Learner class and feeding in LinearRegression\n",
    "## comes from from sklearn.linear_model import LinearRegression.. so i assume all can come from there???\n",
    "learner_rLinearRegression = BaseRRegressor(learner=LinearRegression())\n",
    "ate_r_LinearRegression = learner_rLinearRegression.estimate_ate(X=X[i], treatment = assignment[i], p=propensityScores[i], y=y[i])\n",
    "print('Using the BaseRRegressor class and using Linear Regression:')\n",
    "print(ate_r_LinearRegression)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "learner_p = BaseRRegressor(learner=DecisionTreeRegressor())\n",
    "ate_r = learner_p.estimate_ate(X=X[i], treatment = assignment[i], p=propensityScores[i], y=y[i])\n",
    "print('Using the BaseRRegressor class and using DecisionTree:')\n",
    "print(ate_r_A)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##############################################################################################\n",
    "## what do you do once you have an ATE estimate? then there is a stacking pipeline...?\n",
    "##############################################################################################\n",
    "\n",
    "## there should be an array of ATE estimates. \n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor\n",
    "\n",
    "so the causalml as above outputs ATE estimates and predicitons of a treatment effect for each X_i.\n",
    "\n",
    "STACKING IS : using these predictions against the true dataset in order to create a better estimate for t(). (you keep m() and e() once they are out-of-fold estimated.)\n",
    "\n",
    "LUCKY WE HAVE ENDLESS SIMULATED DATA I GUESS.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get CATE from the causalml package, predict on some holdout set? \n",
    "### FOR THIS YOU NEED TO CREATE A HOLDOUT SET AT ALL!!!!!\n",
    "cate_r = learner_r.fit_predict(X[i], treatment = assignment[i], y=y[i], p=propensityScores[i])\n",
    "treatment[i] \n",
    "\n",
    "### NOT SURE THAT THIS MAKES SENSE AS A PLOT. \n",
    "### you are plotting the PREDICTED treatment effect on the treated. (cate_r)\n",
    "### s\n",
    "\n",
    "\n",
    "alpha=0.2\n",
    "bins=30\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(cate_r, alpha=alpha, bins=bins, label='R Learner')\n",
    "plt.hist(treatment[i], alpha=alpha, bins=bins, label='true treatment effect')\n",
    "plt.title('Distribution of CATE Predictions by Meta Learner')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend().subtract(in_num1, in_num2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### how to get the mean square error from a model? \n",
    "## take actual - predicted and square each one, sum over and take mean. \n",
    "\n",
    "#### YOU ARE MISSING THE FACT THAT THERE ARE NOT AS MANY TREATED. THE PREDICTION ON NON TREATED MAKES NO SENSE. sorry!\n",
    "a = np.array(cate_r)\n",
    "b = np.array(treatment[i])\n",
    "mses = ((a-b)**2).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Create the actual pipeline ('the for loop')\n",
    "2. create a holdout set for each dataset. i think this can be done by taking 1/2 of each dataset ad adding to another test set. \n",
    "3. some plots? some evaluation pipepline? \n",
    "4. figure out how to do the stacking besides the really obvious way. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
