{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "from opossum import UserInterface\n",
    "import matplotlib.pyplot as plt\n",
    "import causalml\n",
    "from causalml.inference.meta import LRSRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from causalml.inference.meta import BaseRRegressor\n",
    "from causalml.propensity import ElasticNetPropensityModel\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create datasets in an array?\n",
    "## for each thing in the data set, take each array and split into 1/3. two new arrays : Test and StackingTrain. \n",
    "# dataSetArray...\n",
    "N = 3000\n",
    "k = 10\n",
    "seed= 5\n",
    "\n",
    "u = UserInterface(N, k, seed=seed, categorical_covariates = None)\n",
    "\n",
    "X=[]\n",
    "assignment=[]\n",
    "y=[]\n",
    "treatment=[]\n",
    "propensityScores=[]\n",
    "\n",
    "def splitArrays(l):\n",
    "    return np.array_split(l, 3)\n",
    "\n",
    "def addDatasets(y_gen, X_gen, assignment_gen, treatment_gen):\n",
    "    X.append(splitArrays(X_gen))\n",
    "    y.append(splitArrays(y_gen))\n",
    "    assignment.append(splitArrays(assignment_gen))\n",
    "    treatment.append(splitArrays(treatment_gen))\n",
    "    \n",
    "    \n",
    "####IMPORTANT: after generating the data, we wish to split the dataset into three parts:\n",
    "## one for training the R learner => 0\n",
    "## one for fitting the OLS stacking model => 1\n",
    "## and one for testing. => 2\n",
    "\n",
    "### because of the nature of the opossum data set output, we will use numpy arrays\n",
    "### after generating a dataset, we add the dataset to an array of datasets. each entry in this (super)array of datasets\n",
    "### contains three sub arrays. one train, one stacking, one test array. \n",
    "\n",
    "## so accessing the training dataset of the second data generating funciton is then: X[1][0]\n",
    "## accessing the treatment vector (true treatment effect) of the first data set, testing data: treatment[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## setupA => difficult nuisance com-ponents and an easy treatment effect function\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = False, \n",
    "                     assignment_prob = 'low', \n",
    "                     constant_pos = False, \n",
    "                     constant_neg = False,\n",
    "                     heterogeneous_pos = True, \n",
    "                     heterogeneous_neg = False, \n",
    "                     no_treatment = False, \n",
    "                     discrete_heterogeneous = False,\n",
    "                     treatment_option_weights = None, \n",
    "                     intensity = 10)\n",
    "\n",
    "y_A, X_A, assignment_A, treatment_A = u.output_data(binary=False, \n",
    "                                               x_y_relation = 'nonlinear_interaction')\n",
    "\n",
    "addDatasets(y_A, X_A, assignment_A, treatment_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## setupB => randomized trial\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = True, \n",
    "                     assignment_prob = 0.5, \n",
    "                     treatment_option_weights = [0.0, 0.0, 0.4, 0.6, 0.0, 0.0],\n",
    "                     intensity = 5)\n",
    "\n",
    "y_B, X_B, assignment_B, treatment_B = u.output_data(binary=False, x_y_relation = 'linear_simple')\n",
    "\n",
    "addDatasets(y_B, X_B, assignment_B, treatment_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "## setupC => easy propensity score and difficult baseline\n",
    "##############################################################################################\n",
    "u.generate_treatment(random_assignment = False, \n",
    "                     assignment_prob = 'low', \n",
    "                     constant_pos = True, \n",
    "                     constant_neg = False,\n",
    "                     heterogeneous_pos = False, \n",
    "                     heterogeneous_neg = False, \n",
    "                     no_treatment = False, \n",
    "                     discrete_heterogeneous = False,\n",
    "                     treatment_option_weights = None, \n",
    "                     intensity = 10)\n",
    "\n",
    "y_C, X_C, assignment_C, treatment_C = u.output_data(binary=False, \n",
    "                                               x_y_relation = 'nonlinear_interaction')\n",
    "\n",
    "addDatasets(y_C, X_C, assignment_C, treatment_C)\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "## setupD => unrelated treatment and control arms???\n",
    "##############################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get propensity scores using CausalML package. (scores for each dataset and save them in an array)\n",
    "propensityScores_train = []\n",
    "\n",
    "for x in range(3):\n",
    "    pm = ElasticNetPropensityModel(n_fold=5, random_state=42)\n",
    "    estimatedpropensityscores = pm.fit_predict(X[x][0], assignment[x][0])\n",
    "    propensityScores_train.append(estimatedpropensityscores)\n",
    "    \n",
    "    \n",
    "#### INFO : the R learner (and others) need propensity scores. there are many ways to compute propensity scores. \n",
    "#### we can do them on the fly when we call the BaseRRegressor, and estimmate_ate .. default is actuallz elasticnetpropensitymodel\n",
    "#### but we can also compute them ourselves. \n",
    "\n",
    "### compute 1 propensity score per dataset for the R learner. we dont need the propensity score for the ols or for the testing. \n",
    "\n",
    "### BUT NOTICE THAT beacuse we dont feed propensitz score into the stacking step, we can see how mis-estimation \n",
    "### of the propensity score can skew the stacking step. Nie and Wager talk about how bias is absorbed into the intercept term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the BaseRRegressor class and using XGB:\n",
      "(array([2.25747814]), array([2.24761998]), array([2.2673363]))\n",
      "Using the BaseRRegressor class and using Linear Regression:\n",
      "(array([2.29305972]), array([2.28541808]), array([2.30070137]))\n",
      "Using the BaseRRegressor class and using DecisionTree:\n",
      "(array([2.01970824]), array([1.88192254]), array([2.15749394]))\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################\n",
    "## R learner, three kinds of base regressors to estimate the treatment effect.\n",
    "## three of these can be used to predict on the stacking set, and then create a stacking coefficient for each\n",
    "##############################################################################################\n",
    "i = 0\n",
    "# R Learner with propensity score input\n",
    "# Calling the Base Learner class and feeding in XGB\n",
    "learner_rXGB = BaseRRegressor(learner=XGBRegressor())\n",
    "ate_r_XGBRegressor = learner_r.estimate_ate(X=X[i][0], treatment = assignment[i][0], p=propensityScores_train[0], y=y[i][0])\n",
    "print('Using the BaseRRegressor class and using XGB:')\n",
    "print(ate_r_XGBRegressor)\n",
    "\n",
    "# Calling the Base Learner class and feeding in LinearRegression\n",
    "## comes from from sklearn.linear_model import LinearRegression.. so i assume all can come from there???\n",
    "learner_rLinearRegression = BaseRRegressor(learner=LinearRegression())\n",
    "ate_r_LinearRegression = learner_rLinearRegression.estimate_ate(X=X[i][0], treatment = assignment[i][0], p=propensityScores_train[0], y=y[i][0])\n",
    "print('Using the BaseRRegressor class and using Linear Regression:')\n",
    "print(ate_r_LinearRegression)\n",
    "\n",
    "learner_decisionTree = BaseRRegressor(learner=DecisionTreeRegressor())\n",
    "ate_r = learner_decisionTree.estimate_ate(X=X[i][0], treatment = assignment[i][0], p=propensityScores_train[0], y=y[i][0])\n",
    "print('Using the BaseRRegressor class and using DecisionTree:')\n",
    "print(ate_r)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##############################################################################################\n",
    "## what do you do once you have an ATE estimate? then there is a stacking pipeline...?\n",
    "##############################################################################################\n",
    "STACKING IS : using these predictions against the true dataset in order to create a better estimate for t(). (you keep m() and e() once they are out-of-fold estimated.)\n",
    "\n",
    "casualml package outputs predictions for y? or prediction on a treatment effect? ->> treatment effect!\n",
    "do the causalml people have anything about the predicted y or does it not really matter that much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_data = []\n",
    "predictions = []\n",
    "## i think its possible to try without propensity scores...\n",
    "\n",
    "cate_rXGB = learner_rXGB.fit_predict(X[i][1], treatment = assignment[i][1], y=y[i][1])\n",
    "cate_linear = learner_rLinearRegression.fit_predict(X[i][1], treatment = assignment[i][1], y=y[i][1])\n",
    "cate_decisionTree = learner_decisionTree.fit_predict(X[i][1], treatment = assignment[i][1], y=y[i][1])\n",
    "\n",
    "## should rename these to something better.\n",
    "predictions.append(cate_rXGB)\n",
    "predictions.append(cate_linear)\n",
    "predictions.append(cate_decisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## so now we have a 3 x 1000 array of predictions on the same dataset, from three different models.\n",
    "## so we can only use some of our dataset to predict on, in order to train a model. \n",
    "\n",
    "## ok you can call fit_predict on all the data, but to fit the ols theres no sense in using all predictions, we can only use the ones we have a true treatment effect on.\n",
    "\n",
    "for x in predictions:\n",
    "    stacking_data.append(x[assignment[i][1] == 1])\n",
    "\n",
    "#treated_elements = X[i][1][assignment[i][1] == 1]\n",
    "#treated_y = y[i][1][assignment[i][1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOT SURE THAT THIS MAKES SENSE AS A PLOT. \n",
    "### you are plotting the PREDICTED treatment effect on the treated. (cate_r)\n",
    "\n",
    "alpha=0.2\n",
    "bins=30\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(cate_r, alpha=alpha, bins=bins, label='R Learner')\n",
    "plt.hist(treatment[i], alpha=alpha, bins=bins, label='true treatment effect')\n",
    "plt.title('Distribution of CATE Predictions by Meta Learner')\n",
    "plt.xlabel('Individual Treatment Effect (ITE/CATE)')\n",
    "plt.ylabel('# of Samples')\n",
    "_=plt.legend().subtract(in_num1, in_num2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### how to get the mean square error from a model? \n",
    "## take actual - predicted and square each one, sum over and take mean. \n",
    "\n",
    "a = np.array(cate_r)\n",
    "b = np.array(treatment[i])\n",
    "mses = ((a-b)**2).mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Create the actual pipeline ('the for loop')\n",
    "\n",
    "3. some plots? some evaluation pipepline? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
